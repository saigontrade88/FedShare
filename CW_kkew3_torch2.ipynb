{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saigontrade88/FedShare/blob/main/CW_kkew3_torch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "gekWr539OUZV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3687e851-85a8-403f-cb73-eed2701b93b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.1+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "VfozTyLNNNr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "FHP9j-BlLsPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import methodcaller\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import operator as op\n",
        "\n",
        "from typing import Union, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import pickle"
      ],
      "metadata": {
        "id": "EGInz3KLLYk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runutils.py"
      ],
      "metadata": {
        "id": "kEr-cwcPLdMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cuda_state(obj):\n",
        "    \"\"\"\n",
        "    Get cuda state of any object.\n",
        "\n",
        "    :param obj: an object (a tensor or an `torch.nn.Module`)\n",
        "    :raise TypeError:\n",
        "    :return: True if the object or the parameter set of the object\n",
        "             is on GPU\n",
        "    \"\"\"\n",
        "    if isinstance(obj, nn.Module):\n",
        "        try:\n",
        "            return next(obj.parameters()).is_cuda\n",
        "        except StopIteration:\n",
        "            return None\n",
        "    elif hasattr(obj, 'is_cuda'):\n",
        "        return obj.is_cuda\n",
        "    else:\n",
        "        raise TypeError('unrecognized type ({}) in args'.format(type(obj)))\n",
        "\n",
        "\n",
        "def is_cuda_consistent(*args):\n",
        "    \"\"\"\n",
        "    See if the cuda states are consistent among variables (of type either\n",
        "    tensors or torch.autograd.Variable). For example,\n",
        "\n",
        "        import torch\n",
        "        from torch.autograd import Variable\n",
        "        import torch.nn as nn\n",
        "\n",
        "        net = nn.Linear(512, 10)\n",
        "        tensor = torch.rand(10, 10).cuda()\n",
        "        assert not is_cuda_consistent(net=net, tensor=tensor)\n",
        "\n",
        "    :param args: the variables to test\n",
        "    :return: True if len(args) == 0 or the cuda states of all elements in args\n",
        "             are consistent; False otherwise\n",
        "    \"\"\"\n",
        "    result = dict()\n",
        "    for v in args:\n",
        "        cur_cuda_state = get_cuda_state(v)\n",
        "        cuda_state = result.get('cuda', cur_cuda_state)\n",
        "        if cur_cuda_state is not cuda_state:\n",
        "            return False\n",
        "        result['cuda'] = cur_cuda_state\n",
        "    return True\n",
        "\n",
        "def make_cuda_consistent(refobj, *args):\n",
        "    \"\"\"\n",
        "    Attempt to make the cuda states of args consistent with that of ``refobj``.\n",
        "    If any element of args is a Variable and the cuda state of the element is\n",
        "    inconsistent with ``refobj``, raise ValueError, since changing the cuda state\n",
        "    of a Variable involves rewrapping it in a new Variable, which changes the\n",
        "    semantics of the code.\n",
        "\n",
        "    :param refobj: either the referential object or the cuda state of the\n",
        "           referential object\n",
        "    :param args: the variables to test\n",
        "    :return: tuple of the same data as ``args`` but on the same device as\n",
        "             ``refobj``\n",
        "    \"\"\"\n",
        "    ref_cuda_state = refobj if type(refobj) is bool else get_cuda_state(refobj)\n",
        "    if ref_cuda_state is None:\n",
        "        raise ValueError('cannot determine the cuda state of `refobj` ({})'\n",
        "                .format(refobj))\n",
        "    move_to_device = methodcaller('cuda' if ref_cuda_state else 'cpu')\n",
        "\n",
        "    result_args = list()\n",
        "    for v in args:\n",
        "        cuda_state = get_cuda_state(v)\n",
        "        if cuda_state != ref_cuda_state:\n",
        "            # if isinstance(v, Variable):\n",
        "            #     raise ValueError('cannot change cuda state of a Variable')\n",
        "            if isinstance(v, nn.Module):\n",
        "                move_to_device(v)\n",
        "            else:\n",
        "                v = move_to_device(v) #v is a Tensor\n",
        "        result_args.append(v)\n",
        "    return tuple(result_args)"
      ],
      "metadata": {
        "id": "hWwETOR1LhAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CW.py"
      ],
      "metadata": {
        "id": "e-UtjXRSL_PU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Util functions"
      ],
      "metadata": {
        "id": "ACaNZGBiMHJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _var2numpy(var):\n",
        "    \"\"\"\n",
        "    Make Variable to numpy array. No transposition will be made.\n",
        "\n",
        "    :param var: Variable instance on whatever device\n",
        "    :type var: Variable\n",
        "    :return: the corresponding numpy array\n",
        "    :rtype: np.ndarray\n",
        "    \"\"\"\n",
        "    return var.data.cpu().numpy()\n",
        "\n",
        "\n",
        "def atanh(x, eps=1e-6):\n",
        "    \"\"\"\n",
        "    The inverse hyperbolic tangent function, missing in pytorch.\n",
        "\n",
        "    :param x: a tensor or a Variable\n",
        "    :param eps: used to enhance numeric stability\n",
        "    :return: :math:`\\\\tanh^{-1}{x}`, of the same type as ``x``\n",
        "    \"\"\"\n",
        "    x = x * (1 - eps)\n",
        "    return 0.5 * torch.log((1.0 + x) / (1.0 - x))\n",
        "\n",
        "def to_tanh_space(x, box):\n",
        "    # type: (Union[Variable, torch.FloatTensor], Tuple[float, float]) -> Union[Variable, torch.FloatTensor]\n",
        "    \"\"\"\n",
        "    Convert a batch of tensors to tanh-space. This method complements the\n",
        "    implementation of the change-of-variable trick in terms of tanh.\n",
        "\n",
        "    :param x: the batch of tensors, of dimension [B x C x H x W]\n",
        "    :param box: a tuple of lower bound and upper bound of the box constraint\n",
        "    :return: the batch of tensors in tanh-space, of the same dimension;\n",
        "             the returned tensor is on the same device as ``x``\n",
        "    \"\"\"\n",
        "    _box_mul = (box[1] - box[0]) * 0.5\n",
        "    _box_plus = (box[1] + box[0]) * 0.5\n",
        "    return atanh((x - _box_plus) / _box_mul)\n",
        "\n",
        "def from_tanh_space(x, box):\n",
        "    # type: (Union[Variable, torch.FloatTensor], Tuple[float, float]) -> Union[Variable, torch.FloatTensor]\n",
        "    \"\"\"\n",
        "    Convert a batch of tensors from tanh-space to oridinary image space.\n",
        "    This method complements the implementation of the change-of-variable trick\n",
        "    in terms of tanh.\n",
        "\n",
        "    :param x: the batch of tensors, of dimension [B x C x H x W]\n",
        "    :param box: a tuple of lower bound and upper bound of the box constraint\n",
        "    :return: the batch of tensors in ordinary image space, of the same\n",
        "             dimension; the returned tensor is on the same device as ``x``\n",
        "    \"\"\"\n",
        "    _box_mul = (box[1] - box[0]) * 0.5\n",
        "    _box_plus = (box[1] + box[0]) * 0.5\n",
        "    return torch.tanh(x) * _box_mul + _box_plus"
      ],
      "metadata": {
        "id": "2xqkW2G5MBR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### L2Adversary class"
      ],
      "metadata": {
        "id": "F4MNsba8MJbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class L2Adversary(object):\n",
        "    \"\"\"\n",
        "    The L2 attack adversary. To enforce the box constraint, the\n",
        "    change-of-variable trick using tanh-space is adopted.\n",
        "\n",
        "    The loss function to optimize:\n",
        "\n",
        "    .. math::\n",
        "        \\\\|\\\\delta\\\\|_2^2 + c \\\\cdot f(x + \\\\delta)\n",
        "\n",
        "    where :math:`f` is defined as\n",
        "\n",
        "    .. math::\n",
        "        f(x') = \\\\max\\\\{0, (\\\\max_{i \\\\ne t}{Z(x')_i} - Z(x')_t) \\\\cdot \\\\tau + \\\\kappa\\\\}\n",
        "\n",
        "    where :math:`\\\\tau` is :math:`+1` if the adversary performs targeted attack;\n",
        "    otherwise it's :math:`-1`.\n",
        "\n",
        "    Usage::\n",
        "\n",
        "        attacker = L2Adversary()\n",
        "        # inputs: a batch of input tensors\n",
        "        # targets: a batch of attack targets\n",
        "        # model: the model to attack\n",
        "        advx = attacker(model, inputs, targets)\n",
        "\n",
        "\n",
        "    The change-of-variable trick\n",
        "    ++++++++++++++++++++++++++++\n",
        "\n",
        "    Let :math:`a` be a proper affine transformation.\n",
        "\n",
        "    1. Given input :math:`x` in image space, map :math:`x` to \"tanh-space\" by\n",
        "\n",
        "    .. math:: \\\\hat{x} = \\\\tanh^{-1}(a^{-1}(x))\n",
        "\n",
        "    2. Optimize an adversarial perturbation :math:`m` without constraint in the\n",
        "    \"tanh-space\", yielding an adversarial example :math:`w = \\\\hat{x} + m`; and\n",
        "\n",
        "    3. Map :math:`w` back to the same image space as the one where :math:`x`\n",
        "    resides:\n",
        "\n",
        "    .. math::\n",
        "        x' = a(\\\\tanh(w))\n",
        "\n",
        "    where :math:`x'` is the adversarial example, and :math:`\\\\delta = x' - x`\n",
        "    is the adversarial perturbation.\n",
        "\n",
        "    Since the composition of affine transformation and hyperbolic tangent is\n",
        "    strictly monotonic, $\\\\delta = 0$ if and only if $m = 0$.\n",
        "\n",
        "    Symbols used in docstring\n",
        "    +++++++++++++++++++++++++\n",
        "\n",
        "    - ``B``: the batch size\n",
        "    - ``C``: the number of channels\n",
        "    - ``H``: the height\n",
        "    - ``W``: the width\n",
        "    - ``M``: the number of classes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, targeted=True, confidence=0.0, c_range=(1e-3, 1e10),\n",
        "                 search_steps=5, max_steps=1000, abort_early=True,\n",
        "                 box=(-1., 1.), optimizer_lr=1e-2, init_rand=False):\n",
        "        \"\"\"\n",
        "        :param targeted: ``True`` to perform targeted attack in ``self.run``\n",
        "               method\n",
        "        :type targeted: bool\n",
        "        :param confidence: the confidence constant, i.e. the $\\\\kappa$ in paper\n",
        "        :type confidence: float\n",
        "        :param c_range: the search range of the constant :math:`c`; should be a\n",
        "               tuple of form (lower_bound, upper_bound)\n",
        "        :type c_range: Tuple[float, float]\n",
        "        :param search_steps: the number of steps to perform binary search of\n",
        "               the constant :math:`c` over ``c_range``\n",
        "        :type search_steps: int\n",
        "        :param max_steps: the maximum number of optimization steps for each\n",
        "               constant :math:`c`\n",
        "        :type max_steps: int\n",
        "        :param abort_early: ``True`` to abort early in process of searching for\n",
        "               :math:`c` when the loss virtually stops increasing\n",
        "        :type abort_early: bool\n",
        "        :param box: a tuple of lower bound and upper bound of the box\n",
        "        :type box: Tuple[float, float]\n",
        "        :param optimizer_lr: the base learning rate of the Adam optimizer used\n",
        "               over the adversarial perturbation in clipped space\n",
        "        :type optimizer_lr: float\n",
        "        :param init_rand: ``True`` to initialize perturbation to small Gaussian;\n",
        "               False is consistent with the original paper, where the\n",
        "               perturbation is initialized to zero\n",
        "        :type init_rand: bool\n",
        "        :rtype: None\n",
        "\n",
        "        Why to make ``box`` default to (-1., 1.) rather than (0., 1.)? TL;DR the\n",
        "        domain of the problem in pytorch is [-1, 1] instead of [0, 1].\n",
        "        According to Xiang Xu (samxucmu@gmail.com)::\n",
        "\n",
        "        > The reason is that in pytorch a transformation is applied first\n",
        "        > before getting the input from the data loader. So image in range [0,1]\n",
        "        > will subtract some mean and divide by std. The normalized input image\n",
        "        > will now be in range [-1,1]. For this implementation, clipping is\n",
        "        > actually performed on the image after normalization, not on the\n",
        "        > original image.\n",
        "\n",
        "        Why to ``optimizer_lr`` default to 1e-2? The optimizer used in Carlini's\n",
        "        code adopts 1e-2. In another pytorch implementation\n",
        "        (https://github.com/rwightman/pytorch-nips2017-attack-example.git),\n",
        "        though, the learning rate is set to 5e-4.\n",
        "        \"\"\"\n",
        "        if len(c_range) != 2:\n",
        "            raise TypeError('c_range ({}) should be of form '\n",
        "                            'tuple([lower_bound, upper_bound])'\n",
        "                            .format(c_range))\n",
        "        if c_range[0] >= c_range[1]:\n",
        "            raise ValueError('c_range lower bound ({}) is expected to be less '\n",
        "                             'than c_range upper bound ({})'.format(*c_range))\n",
        "        if len(box) != 2:\n",
        "            raise TypeError('box ({}) should be of form '\n",
        "                            'tuple([lower_bound, upper_bound])'\n",
        "                            .format(box))\n",
        "        if box[0] >= box[1]:\n",
        "            raise ValueError('box lower bound ({}) is expected to be less than '\n",
        "                             'box upper bound ({})'.format(*box))\n",
        "        self.targeted = targeted\n",
        "        self.confidence = float(confidence)\n",
        "        self.c_range = (float(c_range[0]), float(c_range[1]))\n",
        "        self.binary_search_steps = search_steps\n",
        "        self.max_steps = max_steps\n",
        "        self.abort_early = abort_early\n",
        "        self.ae_tol = 1e-4  # tolerance of early abort\n",
        "        self.box = tuple(map(float, box))  # type: Tuple[float, float]\n",
        "        self.optimizer_lr = optimizer_lr\n",
        "\n",
        "        # `self.init_rand` is not in Carlini's code, it's an attempt in the\n",
        "        # referencing pytorch implementation to improve the quality of attacks.\n",
        "        self.init_rand = init_rand\n",
        "\n",
        "        # Since the larger the `scale_const` is, the more likely a successful\n",
        "        # attack can be found, `self.repeat` guarantees at least attempt the\n",
        "        # largest scale_const once. Moreover, since the optimal criterion is the\n",
        "        # L2 norm of the attack, and the larger `scale_const` is, the larger\n",
        "        # the L2 norm is, thus less optimal, the last attempt at the largest\n",
        "        # `scale_const` won't ruin the optimum ever found.\n",
        "        self.repeat = (self.binary_search_steps >= 10)\n",
        "\n",
        "    def __call__(self, model, inputs, targets, to_numpy=True):\n",
        "        \"\"\"\n",
        "        Produce adversarial examples for ``inputs``.\n",
        "\n",
        "        :param model: the model to attack\n",
        "        :type model: nn.Module\n",
        "        :param inputs: the original images tensor, of dimension [B x C x H x W].\n",
        "               ``inputs`` can be on either CPU or GPU, but it will eventually be\n",
        "               moved to the same device as the one the parameters of ``model``\n",
        "               reside\n",
        "        :type inputs: torch.FloatTensor\n",
        "        :param targets: the original image labels, or the attack targets, of\n",
        "               dimension [B]. If ``self.targeted`` is ``True``, then ``targets``\n",
        "               is treated as the attack targets, otherwise the labels.\n",
        "               ``targets`` can be on either CPU or GPU, but it will eventually\n",
        "               be moved to the same device as the one the parameters of\n",
        "               ``model`` reside\n",
        "        :type targets: torch.LongTensor\n",
        "        :param to_numpy: True to return an `np.ndarray`, otherwise,\n",
        "               `torch.FloatTensor`\n",
        "        :type to_numpy: bool\n",
        "        :return: the adversarial examples on CPU, of dimension [B x C x H x W]\n",
        "        \"\"\"\n",
        "        # sanity check\n",
        "        assert isinstance(model, nn.Module)\n",
        "        assert len(inputs.size()) == 4\n",
        "        assert len(targets.size()) == 1\n",
        "\n",
        "        # get a copy of targets in numpy before moving to GPU, used when doing\n",
        "        # the binary search on `scale_const`\n",
        "        targets_np = targets.clone().cpu().numpy()  # type: np.ndarray\n",
        "\n",
        "        # the type annotations here are used only for type hinting and do\n",
        "        # not indicate the actual type (cuda or cpu); same applies to all codes\n",
        "        # below\n",
        "        inputs = make_cuda_consistent(model, inputs)[0]  # type: torch.FloatTensor\n",
        "        targets = make_cuda_consistent(model, targets)[0]  # type: torch.FloatTensor\n",
        "\n",
        "        # run the model a little bit to get the `num_classes`\n",
        "        num_classes = model(inputs[0][None, :]).size(1)  # type: int\n",
        "\n",
        "        batch_size = inputs.size(0)  # type: int\n",
        "\n",
        "        # `lower_bounds_np`, `upper_bounds_np` and `scale_consts_np` are used\n",
        "        # for binary search of each `scale_const` in the batch. The element-wise\n",
        "        # inquality holds: lower_bounds_np < scale_consts_np <= upper_bounds_np\n",
        "        lower_bounds_np = np.zeros(batch_size)\n",
        "        upper_bounds_np = np.ones(batch_size) * self.c_range[1]\n",
        "        scale_consts_np = np.ones(batch_size) * self.c_range[0]\n",
        "\n",
        "        # Optimal attack to be found.\n",
        "        # The three \"placeholders\" are defined as:\n",
        "        # - `o_best_l2`: the least L2 norms\n",
        "        # - `o_best_l2_ppred`: the perturbed predictions made by the adversarial\n",
        "        #    perturbations with the least L2 norms\n",
        "        # - `o_best_advx`: the underlying adversarial example of\n",
        "        #   `o_best_l2_ppred`\n",
        "        o_best_l2 = np.ones(batch_size) * np.inf\n",
        "        o_best_l2_ppred = -np.ones(batch_size)\n",
        "        o_best_advx = inputs.clone().cpu().numpy()  # type: np.ndarray\n",
        "\n",
        "        # convert `inputs` to tanh-space\n",
        "        inputs_tanh = self._to_tanh_space(inputs)  # type: torch.FloatTensor\n",
        "\n",
        "        #Source: https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html\n",
        "        # inputs_tanh_var = Variable(inputs_tanh_var, requires_grad=False)\n",
        "        inputs_tanh_var = inputs_tanh.detach().clone()\n",
        "\n",
        "\n",
        "        # the one-hot encoding of `targets`\n",
        "        targets_oh = torch.zeros(targets.size() + (num_classes,))  # type: torch.FloatTensor\n",
        "        targets_oh = make_cuda_consistent(model, targets_oh)[0]\n",
        "        targets_oh.scatter_(1, targets.unsqueeze(1), 1.0)\n",
        "        # targets_oh_var = Variable(targets_oh, requires_grad=False)\n",
        "        targets_oh_var = targets_oh.detach().clone()\n",
        "\n",
        "        # the perturbation variable to optimize.\n",
        "        # `pert_tanh` is essentially the adversarial perturbation in tanh-space.\n",
        "        # In Carlini's code it's denoted as `modifier`\n",
        "        pert_tanh = torch.zeros(inputs.size())  # type: torch.FloatTensor\n",
        "        if self.init_rand:\n",
        "            nn.init.normal(pert_tanh, mean=0, std=1e-3)\n",
        "        pert_tanh = make_cuda_consistent(model, pert_tanh)[0]\n",
        "\n",
        "        # pert_tanh_var = Variable(pert_tanh, requires_grad=True)\n",
        "        pert_tanh_var = pert_tanh.clone()\n",
        "\n",
        "\n",
        "        optimizer = optim.Adam([pert_tanh_var], lr=self.optimizer_lr)\n",
        "        for sstep in range(self.binary_search_steps):\n",
        "            if self.repeat and sstep == self.binary_search_steps - 1:\n",
        "                scale_consts_np = upper_bounds_np\n",
        "            scale_consts = torch.from_numpy(np.copy(scale_consts_np)).float()  # type: torch.FloatTensor\n",
        "            scale_consts = make_cuda_consistent(model, scale_consts)[0]\n",
        "\n",
        "            # scale_consts_var = Variable(scale_consts, requires_grad=False)\n",
        "            scale_consts_var = scale_consts.detach().clone()\n",
        "            print(f'Using scale consts: {list(scale_consts_np)}')  # FIXME\n",
        "\n",
        "            # the minimum L2 norms of perturbations found during optimization\n",
        "            best_l2 = np.ones(batch_size) * np.inf\n",
        "            # the perturbed predictions corresponding to `best_l2`, to be used\n",
        "            # in binary search of `scale_const`\n",
        "            best_l2_ppred = -np.ones(batch_size)\n",
        "            # previous (summed) batch loss, to be used in early stopping policy\n",
        "            prev_batch_loss = np.inf  # type: float\n",
        "            for optim_step in range(self.max_steps):\n",
        "                batch_loss, pert_norms_np, pert_outputs_np, advxs_np = \\\n",
        "                    self._optimize(model, optimizer, inputs_tanh_var,\n",
        "                                   pert_tanh_var, targets_oh_var,\n",
        "                                   scale_consts_var)\n",
        "                if optim_step % 10 == 0: print('batch [{}] loss: {}'.format(optim_step, batch_loss))  # FIXME\n",
        "\n",
        "                if self.abort_early and not optim_step % (self.max_steps // 10):\n",
        "                    if batch_loss > prev_batch_loss * (1 - self.ae_tol):\n",
        "                        break\n",
        "                    prev_batch_loss = batch_loss\n",
        "\n",
        "                # update best attack found during optimization\n",
        "                pert_predictions_np = np.argmax(pert_outputs_np, axis=1)\n",
        "                comp_pert_predictions_np = np.argmax(\n",
        "                        self._compensate_confidence(pert_outputs_np,\n",
        "                                                    targets_np),\n",
        "                        axis=1)\n",
        "                for i in range(batch_size):\n",
        "                    l2 = pert_norms_np[i]\n",
        "                    cppred = comp_pert_predictions_np[i]\n",
        "                    ppred = pert_predictions_np[i]\n",
        "                    tlabel = targets_np[i]\n",
        "                    ax = advxs_np[i]\n",
        "                    if self._attack_successful(cppred, tlabel):\n",
        "                        assert cppred == ppred\n",
        "                        if l2 < best_l2[i]:\n",
        "                            best_l2[i] = l2\n",
        "                            best_l2_ppred[i] = ppred\n",
        "                        if l2 < o_best_l2[i]:\n",
        "                            o_best_l2[i] = l2\n",
        "                            o_best_l2_ppred[i] = ppred\n",
        "                            o_best_advx[i] = ax\n",
        "\n",
        "            # binary search of `scale_const`\n",
        "            for i in range(batch_size):\n",
        "                tlabel = targets_np[i]\n",
        "                assert best_l2_ppred[i] == -1 or \\\n",
        "                       self._attack_successful(best_l2_ppred[i], tlabel)\n",
        "                assert o_best_l2_ppred[i] == -1 or \\\n",
        "                       self._attack_successful(o_best_l2_ppred[i], tlabel)\n",
        "                if best_l2_ppred[i] != -1:\n",
        "                    # successful; attempt to lower `scale_const` by halving it\n",
        "                    if scale_consts_np[i] < upper_bounds_np[i]:\n",
        "                        upper_bounds_np[i] = scale_consts_np[i]\n",
        "                    # `upper_bounds_np[i] == c_range[1]` implies no solution\n",
        "                    # found, i.e. upper_bounds_np[i] has never been updated by\n",
        "                    # scale_consts_np[i] until\n",
        "                    # `scale_consts_np[i] > 0.1 * c_range[1]`\n",
        "                    if upper_bounds_np[i] < self.c_range[1] * 0.1:\n",
        "                        scale_consts_np[i] = (lower_bounds_np[i] + upper_bounds_np[i]) / 2\n",
        "                else:\n",
        "                    # failure; multiply `scale_const` by ten if no solution\n",
        "                    # found; otherwise do binary search\n",
        "                    if scale_consts_np[i] > lower_bounds_np[i]:\n",
        "                        lower_bounds_np[i] = scale_consts_np[i]\n",
        "                    if upper_bounds_np[i] < self.c_range[1] * 0.1:\n",
        "                        scale_consts_np[i] = (lower_bounds_np[i] + upper_bounds_np[i]) / 2\n",
        "                    else:\n",
        "                        scale_consts_np[i] *= 10\n",
        "\n",
        "        if not to_numpy:\n",
        "            o_best_advx = torch.from_numpy(o_best_advx).float()\n",
        "        return o_best_advx\n",
        "\n",
        "    def _optimize(self, model, optimizer, inputs_tanh_var, pert_tanh_var,\n",
        "                  targets_oh_var, c_var):\n",
        "        \"\"\"\n",
        "        Optimize for one step.\n",
        "\n",
        "        :param model: the model to attack\n",
        "        :type model: nn.Module\n",
        "        :param optimizer: the Adam optimizer to optimize ``modifier_var``\n",
        "        :type optimizer: optim.Adam\n",
        "        :param inputs_tanh_var: the input images in tanh-space\n",
        "        :type inputs_tanh_var: Variable\n",
        "        :param pert_tanh_var: the perturbation to optimize in tanh-space,\n",
        "               ``pert_tanh_var.requires_grad`` flag must be set to True\n",
        "        :type pert_tanh_var: Variable\n",
        "        :param targets_oh_var: the one-hot encoded target tensor (the attack\n",
        "               targets if self.targeted else image labels)\n",
        "        :type targets_oh_var: Variable\n",
        "        :param c_var: the constant :math:`c` for each perturbation of a batch,\n",
        "               a Variable of FloatTensor of dimension [B]\n",
        "        :type c_var: Variable\n",
        "        :return: the batch loss, squared L2-norm of adversarial perturbations\n",
        "                 (of dimension [B]), the perturbed activations (of dimension\n",
        "                 [B]), the adversarial examples (of dimension [B x C x H x W])\n",
        "        \"\"\"\n",
        "        # the adversarial examples in the image space\n",
        "        # of dimension [B x C x H x W]\n",
        "        advxs_var = self._from_tanh_space(inputs_tanh_var + pert_tanh_var)  # type: Variable\n",
        "        # the perturbed activation before softmax\n",
        "        pert_outputs_var = model(advxs_var)  # type: Variable\n",
        "        # the original inputs\n",
        "        inputs_var = self._from_tanh_space(inputs_tanh_var)  # type: Variable\n",
        "\n",
        "        perts_norm_var = torch.pow(advxs_var - inputs_var, 2)\n",
        "        perts_norm_var = torch.sum(perts_norm_var.view(\n",
        "                perts_norm_var.size(0), -1), 1)\n",
        "\n",
        "        # In Carlini's code, `target_activ_var` is called `real`.\n",
        "        # It should be a Variable of tensor of dimension [B], such that the\n",
        "        # `target_activ_var[i]` is the final activation (right before softmax)\n",
        "        # of the $t$th class, where $t$ is the attack target or the image label\n",
        "        #\n",
        "        # noinspection PyArgumentList\n",
        "        target_activ_var = torch.sum(targets_oh_var * pert_outputs_var, 1)\n",
        "        inf = 1e4  # sadly pytorch does not work with np.inf;\n",
        "                   # 1e4 is also used in Carlini's code\n",
        "        # In Carlini's code, `maxother_activ_var` is called `other`.\n",
        "        # It should be a Variable of tensor of dimension [B], such that the\n",
        "        # `maxother_activ_var[i]` is the maximum final activation of all classes\n",
        "        # other than class $t$, where $t$ is the attack target or the image\n",
        "        # label.\n",
        "        #\n",
        "        # The assertion here ensures (sufficiently yet not necessarily) the\n",
        "        # assumption behind the trick to get `maxother_activ_var` holds, that\n",
        "        # $\\max_{i \\ne t}{o_i} \\ge -\\text{_inf}$, where $t$ is the target and\n",
        "        # $o_i$ the $i$th element along axis=1 of `pert_outputs_var`.\n",
        "        #\n",
        "        # noinspection PyArgumentList\n",
        "        assert (pert_outputs_var.max(1)[0] >= -inf).all(), 'assumption failed'\n",
        "        # noinspection PyArgumentList\n",
        "        maxother_activ_var = torch.max(((1 - targets_oh_var) * pert_outputs_var\n",
        "                                        - targets_oh_var * inf), 1)[0]\n",
        "\n",
        "        # Compute $f(x')$, where $x'$ is the adversarial example in image space.\n",
        "        # The result `f_var` should be of dimension [B]\n",
        "        if self.targeted:\n",
        "            # if targeted, optimize to make `target_activ_var` larger than\n",
        "            # `maxother_activ_var` by `self.confidence`\n",
        "            #\n",
        "            # noinspection PyArgumentList\n",
        "            f_var = torch.clamp(maxother_activ_var - target_activ_var\n",
        "                                + self.confidence, min=0.0)\n",
        "        else:\n",
        "            # if not targeted, optimize to make `maxother_activ_var` larger than\n",
        "            # `target_activ_var` (the ground truth image labels) by\n",
        "            # `self.confidence`\n",
        "            #\n",
        "            # noinspection PyArgumentList\n",
        "            f_var = torch.clamp(target_activ_var - maxother_activ_var\n",
        "                                + self.confidence, min=0.0)\n",
        "        # the total loss of current batch, should be of dimension [1]\n",
        "        batch_loss_var = torch.sum(perts_norm_var + c_var * f_var)  # type: Variable\n",
        "        # print(type(batch_loss_var))\n",
        "\n",
        "        # Do optimization for one step\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss_var.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Make some records in python/numpy on CPU\n",
        "\n",
        "        batch_loss = batch_loss_var.data # type: float\n",
        "        pert_norms_np = _var2numpy(perts_norm_var)\n",
        "        pert_outputs_np = _var2numpy(pert_outputs_var)\n",
        "        advxs_np = _var2numpy(advxs_var)\n",
        "        return batch_loss, pert_norms_np, pert_outputs_np, advxs_np\n",
        "\n",
        "    def _attack_successful(self, prediction, target):\n",
        "        \"\"\"\n",
        "        See whether the underlying attack is successful.\n",
        "\n",
        "        :param prediction: the prediction of the model on an input\n",
        "        :type prediction: int\n",
        "        :param target: either the attack target or the ground-truth image label\n",
        "        :type target: int\n",
        "        :return: ``True`` if the attack is successful\n",
        "        :rtype: bool\n",
        "        \"\"\"\n",
        "        if self.targeted:\n",
        "            return prediction == target\n",
        "        else:\n",
        "            return prediction != target\n",
        "\n",
        "    # noinspection PyUnresolvedReferences\n",
        "    def _compensate_confidence(self, outputs, targets):\n",
        "        \"\"\"\n",
        "        Compensate for ``self.confidence`` and returns a new weighted sum\n",
        "        vector.\n",
        "\n",
        "        :param outputs: the weighted sum right before the last layer softmax\n",
        "               normalization, of dimension [B x M]\n",
        "        :type outputs: np.ndarray\n",
        "        :param targets: either the attack targets or the real image labels,\n",
        "               depending on whether or not ``self.targeted``, of dimension [B]\n",
        "        :type targets: np.ndarray\n",
        "        :return: the compensated weighted sum of dimension [B x M]\n",
        "        :rtype: np.ndarray\n",
        "        \"\"\"\n",
        "        outputs_comp = np.copy(outputs)\n",
        "        rng = np.arange(targets.shape[0])\n",
        "        if self.targeted:\n",
        "            # for each image $i$:\n",
        "            # if targeted, `outputs[i, target_onehot]` should be larger than\n",
        "            # `max(outputs[i, ~target_onehot])` by `self.confidence`\n",
        "            outputs_comp[rng, targets] -= self.confidence\n",
        "        else:\n",
        "            # for each image $i$:\n",
        "            # if not targeted, `max(outputs[i, ~target_onehot]` should be larger\n",
        "            # than `outputs[i, target_onehot]` (the ground truth image labels)\n",
        "            # by `self.confidence`\n",
        "            outputs_comp[rng, targets] += self.confidence\n",
        "        return outputs_comp\n",
        "\n",
        "    def _to_tanh_space(self, x):\n",
        "        \"\"\"\n",
        "        Convert a batch of tensors to tanh-space.\n",
        "\n",
        "        :param x: the batch of tensors, of dimension [B x C x H x W]\n",
        "        :return: the batch of tensors in tanh-space, of the same dimension\n",
        "        \"\"\"\n",
        "        return to_tanh_space(x, self.box)\n",
        "\n",
        "    def _from_tanh_space(self, x):\n",
        "        \"\"\"\n",
        "        Convert a batch of tensors from tanh-space to input space.\n",
        "\n",
        "        :param x: the batch of tensors, of dimension [B x C x H x W]\n",
        "        :return: the batch of tensors in tanh-space, of the same dimension;\n",
        "                 the returned tensor is on the same device as ``x``\n",
        "        \"\"\"\n",
        "        return from_tanh_space(x, self.box)\n"
      ],
      "metadata": {
        "id": "G_Z6Gg70MLs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader"
      ],
      "metadata": {
        "id": "Yi8hDEBIL50_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source and reference\n",
        "\n",
        "#https://github.com/kkew3/pytorch-cw2\n",
        "\n",
        "#You need to download the MNIST dataset. Reference:\n",
        "\n",
        "#1) FedShare_AT_v14_PGD_Gaussian_SL_noniid_automation.py\n",
        "\n",
        "#2) The Skitlearn and Pytorch textbook\n",
        "\n",
        "# Need to install a virtual enviroment\n",
        "#\n",
        "#### Insert your code here #########\n",
        "\n",
        "image_path = '/content'\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "mnist_train_dataset = torchvision.datasets.MNIST(\n",
        "            root=image_path,\n",
        "            train=True,\n",
        "            download=True,\n",
        "            transform=transform)\n",
        "mnist_test_dataset = torchvision.datasets.MNIST(\n",
        "    root=image_path, train=False, download=False,\n",
        "    transform=transform)\n",
        "\n",
        "batch_size = 100\n",
        "torch.manual_seed(1)\n",
        "train_dl = DataLoader(mnist_train_dataset,\n",
        "                      batch_size, shuffle=True)\n",
        "\n",
        "#create a dataloader based on MNIST dataset, with a small input of 100 training examples only.\n",
        "# The ultimate goal is to save the whole training set of MNIST dataset of 60,000 examples.\n",
        "\n",
        "\n",
        "##### End your code##################\n",
        "#mean and std are both 3-tuples of floats.\n",
        "#mean=(0.0, 0.13092535192648502) # for colorful image, a RGB image. MNIST is gray scale, you need a float value\n",
        "\n",
        "#std=(0.1, 0.3084485240270358) # you need a float value.\n",
        "\n",
        "#inputs_box = (min((0 - m) / s for m, s in zip(mean, std)),\n",
        "#              max((1 - m) / s for m, s in zip(mean, std)))\n",
        "\n",
        "# an untargeted adversary\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5H3NFmJLL0-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79faf04f-8d7a-41d6-811c-11a118e493b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /content/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 93657763.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/train-images-idx3-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /content/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 91492215.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/train-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /content/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 27555328.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 22897270.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = torch.Tensor([10])\n",
        "num_classes.shape\n",
        "num_classes.requires_grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DLPiLjbQaPh",
        "outputId": "35f45bf8-8025-46f5-b2e0-aad15bac77ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "kKh03qxQQ9Bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, targets = next(iter(train_dl))\n",
        "\n",
        "print(f'inputs.size {inputs.shape}')\n",
        "print(f'targets.size {targets.shape}')\n",
        "\n",
        "print(f'targets.size {targets.size()}')\n",
        "\n",
        "#### Insert your code here #########\n",
        "\n",
        "#Need to define the model named 'net'\n",
        "#Need to define the variable 'targets'\n",
        "\n",
        "class net(nn.Module): #this is a deep learning's model. it is a nested function. Pytorch they have built-in library to define.\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Conv2d(1, 100, (3, 3)),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(100, 200, (3, 3)),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(200*(28-4)*(28-4), 10),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "\n",
        "# targets == [1] #mnist, we have 10 classes\n",
        "\n",
        "model = net().to(device)\n",
        "##### End your code##################\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1WtJHjVNUou",
        "outputId": "4b4ea93c-59ec-445d-b8ea-6c4cde9635a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.size torch.Size([100, 1, 28, 28])\n",
            "targets.size torch.Size([100])\n",
            "targets.size torch.Size([100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debug"
      ],
      "metadata": {
        "id": "Kj1bvjGjS0yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with torch.no_grad():\n",
        "#   pred = model(inputs[0][None, :])\n",
        "# print(pred.shape)\n",
        "# print(pred)\n",
        "# print(pred.size())\n",
        "# print(pred.size(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "PrVHy3GDOfaL",
        "outputId": "12f946d5-ae23-44bf-d1e7-4caf0433b578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5ffc58b8d559>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-24e987c6499c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# targets == [1] #mnist, we have 10 classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper_CUDA___slow_conv2d_forward)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attack"
      ],
      "metadata": {
        "id": "tKX8c8BgMilN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "adversary = L2Adversary(targeted=True,\n",
        "                confidence=0.0, c_range=(1e-3, 1e10),\n",
        "                 search_steps=10,\n",
        "                 max_steps=1000,\n",
        "                 abort_early=True,\n",
        "                 box=(-1., 1.),\n",
        "                 optimizer_lr=5e-4,\n",
        "                 init_rand=False)"
      ],
      "metadata": {
        "id": "drdROcp0UD87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## It takes 26 minutes on CPU!"
      ],
      "metadata": {
        "id": "Eo35sZ64X9J9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## It takes 21 seconds on GPU!!!"
      ],
      "metadata": {
        "id": "hLc9GD2NaDqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adversary = L2Adversary(targeted=False,\n",
        "                            confidence=0.0,\n",
        "                            search_steps=10,\n",
        "                            optimizer_lr=1e-4)\n",
        "\n",
        "adversarial_examples = adversary(model,\n",
        "                                 inputs,\n",
        "                                 targets,\n",
        "                                 to_numpy=True)\n",
        "\n",
        "#### Insert your code here #########\n",
        "\n",
        "#Need to save 'adversarial_examples' variable into a file with extension npy (Numpy)\n",
        "\n",
        "with open('adversarial_examples.npy', 'wb') as f:\n",
        "     pickle.dump(adversarial_examples, f)\n",
        "\n",
        "##### End your code##################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mRvgtrrOZfB",
        "outputId": "672eea13-bbe6-4508-fced-4905711baaee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using scale consts: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
            "batch [0] loss: 0.0003688335418701172\n",
            "batch [10] loss: 0.0003688335418701172\n",
            "batch [20] loss: 0.0003688335418701172\n",
            "batch [30] loss: 0.0003688335418701172\n",
            "batch [40] loss: 0.0003688335418701172\n",
            "batch [50] loss: 0.0003688335418701172\n",
            "batch [60] loss: 0.0003688335418701172\n",
            "batch [70] loss: 0.0003688335418701172\n",
            "batch [80] loss: 0.0003688335418701172\n",
            "batch [90] loss: 0.0003688335418701172\n",
            "batch [100] loss: 0.0003688335418701172\n",
            "Using scale consts: [0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.01, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.01, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.01, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.01, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.01, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.01, 0.0005, 0.0005, 0.01, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.01, 0.0005, 0.0005, 0.0005, 0.0005, 0.01, 0.01, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005]\n",
            "batch [0] loss: 0.0036883351858705282\n",
            "batch [10] loss: 0.0036883351858705282\n",
            "batch [20] loss: 0.0036883351858705282\n",
            "batch [30] loss: 0.0036883351858705282\n",
            "batch [40] loss: 0.0036883351858705282\n",
            "batch [50] loss: 0.0036883351858705282\n",
            "batch [60] loss: 0.0036883351858705282\n",
            "batch [70] loss: 0.0036883351858705282\n",
            "batch [80] loss: 0.0036883351858705282\n",
            "batch [90] loss: 0.0036883351858705282\n",
            "batch [100] loss: 0.0036883351858705282\n",
            "Using scale consts: [0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.1, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.1, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.1, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.1, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.1, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.1, 0.00025, 0.00025, 0.1, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.1, 0.00025, 0.00025, 0.00025, 0.00025, 0.1, 0.1, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025, 0.00025]\n",
            "batch [0] loss: 0.03688335046172142\n",
            "batch [10] loss: 0.03688335046172142\n",
            "batch [20] loss: 0.03688335046172142\n",
            "batch [30] loss: 0.03688335046172142\n",
            "batch [40] loss: 0.03688335046172142\n",
            "batch [50] loss: 0.03688335046172142\n",
            "batch [60] loss: 0.03688335046172142\n",
            "batch [70] loss: 0.03688335046172142\n",
            "batch [80] loss: 0.03688335046172142\n",
            "batch [90] loss: 0.03688335046172142\n",
            "batch [100] loss: 0.03688335046172142\n",
            "Using scale consts: [0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 1.0, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 1.0, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 1.0, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 1.0, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 1.0, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 1.0, 0.000125, 0.000125, 1.0, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 1.0, 0.000125, 0.000125, 0.000125, 0.000125, 1.0, 1.0, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125, 0.000125]\n",
            "batch [0] loss: 0.3688334822654724\n",
            "batch [10] loss: 0.3688334822654724\n",
            "batch [20] loss: 0.3688334822654724\n",
            "batch [30] loss: 0.3688334822654724\n",
            "batch [40] loss: 0.3688334822654724\n",
            "batch [50] loss: 0.3688334822654724\n",
            "batch [60] loss: 0.3688334822654724\n",
            "batch [70] loss: 0.3688334822654724\n",
            "batch [80] loss: 0.3688334822654724\n",
            "batch [90] loss: 0.3688334822654724\n",
            "batch [100] loss: 0.3688334822654724\n",
            "Using scale consts: [6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 10.0, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 10.0, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 10.0, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 10.0, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 10.0, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 10.0, 6.25e-05, 6.25e-05, 10.0, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 10.0, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 10.0, 10.0, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05, 6.25e-05]\n",
            "batch [0] loss: 3.6883349418640137\n",
            "batch [10] loss: 3.6883349418640137\n",
            "batch [20] loss: 3.6883349418640137\n",
            "batch [30] loss: 3.6883349418640137\n",
            "batch [40] loss: 3.6883349418640137\n",
            "batch [50] loss: 3.6883349418640137\n",
            "batch [60] loss: 3.6883349418640137\n",
            "batch [70] loss: 3.6883349418640137\n",
            "batch [80] loss: 3.6883349418640137\n",
            "batch [90] loss: 3.6883349418640137\n",
            "batch [100] loss: 3.6883349418640137\n",
            "Using scale consts: [3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 100.0, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 100.0, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 100.0, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 100.0, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 100.0, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 100.0, 3.125e-05, 3.125e-05, 100.0, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 100.0, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 100.0, 100.0, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05, 3.125e-05]\n",
            "batch [0] loss: 36.88335037231445\n",
            "batch [10] loss: 36.88335037231445\n",
            "batch [20] loss: 36.88335037231445\n",
            "batch [30] loss: 36.88335037231445\n",
            "batch [40] loss: 36.88335037231445\n",
            "batch [50] loss: 36.88335037231445\n",
            "batch [60] loss: 36.88335037231445\n",
            "batch [70] loss: 36.88335037231445\n",
            "batch [80] loss: 36.88335037231445\n",
            "batch [90] loss: 36.88335037231445\n",
            "batch [100] loss: 36.88335037231445\n",
            "Using scale consts: [1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1000.0, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1000.0, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1000.0, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1000.0, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1000.0, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1000.0, 1.5625e-05, 1.5625e-05, 1000.0, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1000.0, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1000.0, 1000.0, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05, 1.5625e-05]\n",
            "batch [0] loss: 368.83349609375\n",
            "batch [10] loss: 368.83349609375\n",
            "batch [20] loss: 368.83349609375\n",
            "batch [30] loss: 368.83349609375\n",
            "batch [40] loss: 368.83349609375\n",
            "batch [50] loss: 368.83349609375\n",
            "batch [60] loss: 368.83349609375\n",
            "batch [70] loss: 368.83349609375\n",
            "batch [80] loss: 368.83349609375\n",
            "batch [90] loss: 368.83349609375\n",
            "batch [100] loss: 368.83349609375\n",
            "Using scale consts: [7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 10000.0, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 10000.0, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 10000.0, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 10000.0, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 10000.0, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 10000.0, 7.8125e-06, 7.8125e-06, 10000.0, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 10000.0, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 10000.0, 10000.0, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06, 7.8125e-06]\n",
            "batch [0] loss: 3688.3349609375\n",
            "batch [10] loss: 3688.3349609375\n",
            "batch [20] loss: 3688.3349609375\n",
            "batch [30] loss: 3688.3349609375\n",
            "batch [40] loss: 3688.3349609375\n",
            "batch [50] loss: 3688.3349609375\n",
            "batch [60] loss: 3688.3349609375\n",
            "batch [70] loss: 3688.3349609375\n",
            "batch [80] loss: 3688.3349609375\n",
            "batch [90] loss: 3688.3349609375\n",
            "batch [100] loss: 3688.3349609375\n",
            "Using scale consts: [3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 100000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 100000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 100000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 100000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 100000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 100000.0, 3.90625e-06, 3.90625e-06, 100000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 100000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 100000.0, 100000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06]\n",
            "batch [0] loss: 36883.3515625\n",
            "batch [10] loss: 36883.3515625\n",
            "batch [20] loss: 36883.3515625\n",
            "batch [30] loss: 36883.3515625\n",
            "batch [40] loss: 36883.3515625\n",
            "batch [50] loss: 36883.3515625\n",
            "batch [60] loss: 36883.3515625\n",
            "batch [70] loss: 36883.3515625\n",
            "batch [80] loss: 36883.3515625\n",
            "batch [90] loss: 36883.3515625\n",
            "batch [100] loss: 36883.3515625\n",
            "Using scale consts: [3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 10000000000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 10000000000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 10000000000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 10000000000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 10000000000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 10000000000.0, 3.90625e-06, 3.90625e-06, 10000000000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 10000000000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 10000000000.0, 10000000000.0, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06, 3.90625e-06]\n",
            "batch [0] loss: 3688334848.0\n",
            "batch [10] loss: 3688334848.0\n",
            "batch [20] loss: 3688334848.0\n",
            "batch [30] loss: 3688334848.0\n",
            "batch [40] loss: 3688334848.0\n",
            "batch [50] loss: 3688334848.0\n",
            "batch [60] loss: 3688334848.0\n",
            "batch [70] loss: 3688334848.0\n",
            "batch [80] loss: 3688334848.0\n",
            "batch [90] loss: 3688334848.0\n",
            "batch [100] loss: 3688334848.0\n"
          ]
        }
      ]
    }
  ]
}